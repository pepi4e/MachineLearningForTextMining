{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NONNEGATIVE MATRIX FACTORISATION FOR TOPIC EXTRACTION\n",
    "## - TOPIC EXTRACTION FROM DOCUMENTS -\n",
    "\n",
    "The goal is to study the use of nonnegative matrix factorisation (NMF) for topic extraction from a dataset of text documents. The rationale is to interpret each extracted NMF component as being associated with a specific topic.\n",
    "\n",
    "\n",
    "Study and test the following script (introduced  on http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresToVector(vectorizer=None, random_state=None):\n",
    "    # Take default parameters from assignment script\n",
    "    if vectorizer is None:\n",
    "        n_features = 1000\n",
    "        _vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                max_features=n_features, stop_words='english')\n",
    "    else:\n",
    "        _vectorizer = vectorizer\n",
    "    # For results replicability\n",
    "    if random_state is None:\n",
    "        _random_state = 1\n",
    "    else:\n",
    "        _random_state = random_state\n",
    "        \n",
    "    # Load data\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=_random_state,\n",
    "                                 remove=('headers', 'footers', 'quotes'))\n",
    "    n_samples = 2000\n",
    "    data_samples = dataset.data[:n_samples]        \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    # Vectorize\n",
    "    print(\"Vectorizing...\")\n",
    "    t0 = time()\n",
    "    features = _vectorizer.fit_transform(data_samples)\n",
    "    names = _vectorizer.get_feature_names()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    return features, names\n",
    "    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "def NmfModel(features, vectorizer=None, random_state=None, \n",
    "             beta_loss=None, init=None, W=None, H=None, K=None,\n",
    "             verbose=False):\n",
    "    \n",
    "    if K is None:\n",
    "        n_components = 10 \n",
    "    else:\n",
    "        n_components = K\n",
    "        \n",
    "    if vectorizer is None: \n",
    "        _vectorizer = 'tf_idf'\n",
    "    else:\n",
    "        _vectorizer = vectorizer\n",
    "        \n",
    "    if random_state is None: \n",
    "        _random_state = 1         \n",
    "    else:\n",
    "        _random_state = random_state\n",
    "        \n",
    "    if beta_loss is None:\n",
    "        _solver = 'cd' \n",
    "    else:\n",
    "        _solver = 'mu'\n",
    "    \n",
    "    if beta_loss is None:\n",
    "        _beta_loss = 'frobenius' \n",
    "    else:\n",
    "        _beta_loss = beta_loss\n",
    "    \n",
    "    if init is None:\n",
    "        _init = 'random' \n",
    "    else:\n",
    "        _init = init\n",
    "    \n",
    "    n_samples = features.shape[0]\n",
    "    n_features = features.shape[1]\n",
    "    print(\"Fitting the NMF model (\"+_beta_loss+\" norm) with \"+_vectorizer+\" features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "    \n",
    "    t0 = time()\n",
    "    if init is None:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  init= _init,\n",
    "                  solver = _solver,\n",
    "                  beta_loss = _beta_loss,\n",
    "                  alpha=.1, l1_ratio=.5, verbose=verbose).fit(features)\n",
    "    else:\n",
    "        nmf = NMF(n_components=n_components, \n",
    "                  random_state=_random_state,\n",
    "                  init= _init,\n",
    "                  solver = _solver,\n",
    "                  beta_loss = _beta_loss,\n",
    "                  alpha=.1, l1_ratio=.5, verbose=verbose)\n",
    "        nmf.fit_transform(features, W=W, H=H)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    return nmf\n",
    "\n",
    "def runNmf(vectorizer=None, vectorizerName=None, random_state=None, beta_loss=None, \n",
    "           init=None, W=None, H=None, K=None, verbose=False, print_results=False,\n",
    "           n_top_words=10):\n",
    "    features, names = featuresToVector(vectorizer=vectorizer)\n",
    "    nmf = NmfModel(features, vectorizer=vectorizerName,\n",
    "                                random_state=random_state, \n",
    "                                beta_loss=beta_loss, init=init,\n",
    "                                W=W, H=H, K=K, verbose=verbose)\n",
    "    if print_results:\n",
    "        print(\"\\nTopics in NMF model (\"+_beta_loss+\" norm):\")\n",
    "        print_top_words(nmf, names, n_top_words)\n",
    "    \n",
    "    return nmf, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test and comment on the effect of varying the initialisation, especially using random nonnegative values as initial guesses (for W and H coefficients, using the notations introduced during the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.150s.\n",
      "Vectorizing...\n",
      "done in 0.271s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.068s.\n",
      "Converged after 30 iterations.\n",
      "Frobenius norm of matrix difference: 42.14133796612238\n"
     ]
    }
   ],
   "source": [
    "nmf, names = runNmf(random_state=0, beta_loss='frobenius',\n",
    "                    init='nndsvda', n_top_words=20)\n",
    "print('Converged after ' + str(nmf.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: ' \n",
    "      + str(nmf.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.157s.\n",
      "Vectorizing...\n",
      "done in 0.263s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.086s.\n",
      "Converged after 40 iterations.\n",
      "Frobenius norm of matrix difference: 42.14711940431842\n"
     ]
    }
   ],
   "source": [
    "nmf, names = runNmf(random_state=0, beta_loss='frobenius',\n",
    "                    init='nndsvdar', n_top_words=20)\n",
    "print('Converged after ' + str(nmf.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: ' \n",
    "      + str(nmf.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.171s.\n",
      "Vectorizing...\n",
      "done in 0.267s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.084s.\n",
      "Converged after 50 iterations.\n",
      "Frobenius norm of matrix difference: 42.17995382807882\n"
     ]
    }
   ],
   "source": [
    "nmf, names = runNmf(random_state=0, beta_loss='frobenius', \n",
    "                    init='random', n_top_words=20)\n",
    "print('Converged after ' + str(nmf.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: '\n",
    "      + str(nmf.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different initialisation configuration were tested, while mantaining the same cost function (frobenius norm) and using multiplicative update rules. The 'nndsvda' initializiation, as from the scikit documentation, performs a Nonnegative Double Singular Value Decomposition on the features and then fills zero values with the average value of the features matrix. The 'nndsvdar' initializiation performs the same operation, but fills zeros with very small random values. Finally, the random initialization creates two non-negative random matrices properly scaled.\n",
    "\n",
    "Of all the three approaches, the one that showed the best results was undoubtly the 'nndsvda', which was able to converge with only 30 iterations and provided the lowest error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare and comment on the difference between the results obtained with $l_{2}$ cost compared to the generalised Kullback-Leibler cost.\n",
    "\n",
    "$l_{2}$ and frobenius norm are just two different words for the same regularization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.161s.\n",
      "Vectorizing...\n",
      "done in 0.258s.\n",
      "Fitting the NMF model (frobenius norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.070s.\n",
      "Converged after 30 iterations.\n",
      "Frobenius norm of matrix difference: 42.14133796612238\n"
     ]
    }
   ],
   "source": [
    "nmf_f, names_f = runNmf(random_state=0, beta_loss='frobenius',\n",
    "                    init='nndsvda')\n",
    "print('Converged after ' + str(nmf_f.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: ' \n",
    "      + str(nmf_f.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.144s.\n",
      "Vectorizing...\n",
      "done in 0.264s.\n",
      "Fitting the NMF model (kullback-leibler norm) with tf_idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.981s.\n",
      "Converged after 110 iterations.\n",
      "Frobenius norm of matrix difference: 211.17742134247516\n"
     ]
    }
   ],
   "source": [
    "nmf_k, names_k = runNmf(random_state=0, beta_loss='kullback-leibler',\n",
    "                    init='nndsvda')\n",
    "print('Converged after ' + str(nmf_k.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: ' \n",
    "      + str(nmf_k.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Lieber cost seems to perform worse than the $ l_{2} $ cost: it needs 3.5 times the iterations to converge, and has an error five times higher. Let us try and print also the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics obtained from NFM with l2 cost:\n",
      "Topic #0: just people don think like know good time make way really say right ve did\n",
      "Topic #1: windows use dos using window program card help software pc drivers os application video running\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary\n",
      "Topic #3: thanks know does advance mail info hi interested email anybody like list send information appreciated\n",
      "Topic #4: 00 car sale 10 price condition new card cars offer 250 12 asking 15 sell\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc article information pub hope mac\n",
      "Topic #6: file files problem format win sound read pub ftp save create site running self copy\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers\n",
      "Topic #8: drive drives hard disk floppy mac software mb controller scsi computer rom apple internal power\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa encrypted communications law\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Topics obtained from NFM with l2 cost:')\n",
    "print_top_words(nmf_f, names_f, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics obtained from NFM with Kullback-Leibler cost:\n",
      "Topic #0: people did right like time things said look say don just know want real work\n",
      "Topic #1: using windows help work use need hi thanks looking software pc video running card used\n",
      "Topic #2: does true god mean say says believe read matter fact point people word example particular\n",
      "Topic #3: know thanks like mail interested does post send list new don want wondering hear email\n",
      "Topic #4: new 10 old sale 20 offer 15 30 weeks 16 power test check 25 11\n",
      "Topic #5: number states com control free university government space research including general white women data used\n",
      "Topic #6: edu remember file try soon sun problem article reading available short couldn written copy mentioned\n",
      "Topic #7: year won second world team time win game maybe news play season games bad series\n",
      "Topic #8: think people make hard means drive write don read problems need similar problem case actually\n",
      "Topic #9: just use sure way good got doesn like wrong want going don kind doing need\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Topics obtained from NFM with Kullback-Leibler cost:')\n",
    "print_top_words(nmf_k, names_k, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found topics seem to be similar (religion, business, games...), but also from this representation we can see that the l2 cost produces more accurate results: in topics obtained with l2 cost, all words within the same topic seem to be specific to it, whilst in the topics obtained with Kullback-Leibler cost there seems to be less precision. As an example, we can look at topic #2, where both algorithms find the same topic, but the most important words found by using the l2 cost are much more representative than the ones found using the Kullback-Leibler one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test and comment on the results obtained using a simpler term-frequency representation as input (as opposed to the TF-IDF representation considered in the code above) when considering the Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.154s.\n",
      "Vectorizing...\n",
      "done in 0.282s.\n",
      "Fitting the NMF model (kullback-leibler norm) with TermFrequencyVectorizer features, n_samples=2000 and n_features=1000...\n",
      "done in 2.174s.\n",
      "Converged after 120 iterations.\n",
      "Frobenius norm of matrix difference: 213.63610577925184\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english', use_idf=False)    \n",
    "nmf, names = runNmf(random_state=0, beta_loss='kullback-leibler',\n",
    "                    vectorizer=vectorizer, vectorizerName='TermFrequencyVectorizer', init='nndsvda')\n",
    "print('Converged after ' + str(nmf.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: ' \n",
    "      + str(nmf.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.146s.\n",
      "Vectorizing...\n",
      "done in 0.260s.\n",
      "Fitting the NMF model (kullback-leibler norm) with CountVectorizer features, n_samples=2000 and n_features=1000...\n",
      "done in 1.745s.\n",
      "Converged after 120 iterations.\n",
      "Frobenius norm of matrix difference: 592.1368392281813\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')    \n",
    "nmf, names = runNmf(random_state=0, beta_loss='kullback-leibler',\n",
    "                    vectorizer=vectorizer, vectorizerName='CountVectorizer', init='nndsvda')\n",
    "print('Converged after ' + str(nmf.n_iter_) + ' iterations.')\n",
    "print('Frobenius norm of matrix difference: ' \n",
    "      + str(nmf.reconstruction_err_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two simpler representations were tested, both the simple Term Frequency representation and the simple Count of tokens. However, neither of them seems to produce better results: if compared to the previous Tf-Idf representation, both of them need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CUSTOM NMF IMPLEMENTATION -\n",
    "Implement the multiplicative update rules (derived from the majorisation-minimisation approach) for NMF estimation with β divergences, including the case β = 1 (generalised Kullback-Liebler divergence). Ensure that:\n",
    "- You can easily choose a custom initialisation for the W and H matrices;\n",
    "- You can set a custom number of iteration;\n",
    "- You can monitor the behaviour of the loss function across the iterations and that it is readily decreasing.\n",
    "\n",
    "Compare your implementation with the one offered by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def custom_NMF(V, k=2, W=None, H=None, max_iter=50, beta=0, tol=0.1, verbose=False):\n",
    "    \n",
    "    def _beta_div(V,W,H,beta):\n",
    "        div = 0\n",
    "        # Update beta_divergence\n",
    "        if beta == 1: \n",
    "            func = _kullback_leiber\n",
    "        elif beta == 0: \n",
    "            func = _itakura_saito\n",
    "        else: \n",
    "            func = _euclidean_distance\n",
    "        WH = np.dot(W, H)\n",
    "        for i in range(V.shape[0]):\n",
    "            for j in range(V.shape[1]):\n",
    "                x = V[i][j]\n",
    "                if x == 0:\n",
    "                    x = np.finfo(np.double).tiny\n",
    "                y = WH[i][j]\n",
    "                div += func(x,y,beta)\n",
    "        return div\n",
    "\n",
    "    # Generalized Kullback-Leibler divergence -> d = x*log(x/y) - x + y\n",
    "    def _kullback_leiber(x,y,beta):\n",
    "        return x*np.log(x/y) - x + y\n",
    "\n",
    "    # Itakura-Saito divergence -> d = (x/y) - log(x/y) -1\n",
    "    def _itakura_saito(x,y,beta):\n",
    "        return (x/y)-np.log(x/y) - 1\n",
    "    \n",
    "    # Euclidean distance -> d = (1/beta(beta-1))(x^beta + (beta-1)y^beta - beta*x*y^beta-1)    \n",
    "    def _euclidean_distance(x,y,beta):\n",
    "        return (1/(beta*(beta-1)))*(np.pow(x,beta) + (beta-1)*np.pow(y,beta) - beta*x*np.pow(y,beta-1))\n",
    "\n",
    "    if V is None:\n",
    "        raise ValueError(\"Please provide initial Matrix V.\")\n",
    "\n",
    "    if W is None:\n",
    "        W = np.random.rand(V.shape[0],k)\n",
    "        \n",
    "    if H is None:\n",
    "        H = np.random.rand(k,V.shape[1])\n",
    "            \n",
    "    # Setup initial error\n",
    "    init_error = _beta_div(V,W,H,beta)\n",
    "    if verbose:\n",
    "        print(\"Initial error: \"+str(init_error))\n",
    "    error = init_error\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "    \n",
    "        # Tests with whole matrix : multiply = O | dot = *\n",
    "        mur_up = np.dot(W.T, np.multiply(np.power(np.dot(W,H),beta-2), V))\n",
    "        mur_down = np.dot(W.T, np.power(np.dot(W,H),beta-1))\n",
    "        mur = np.divide(mur_up, mur_down)\n",
    "        H = np.multiply(H, mur)\n",
    "        \n",
    "        mur_up = np.dot(np.multiply(np.power(np.dot(W,H),beta-2), V),H.T)\n",
    "        mur_down = np.dot(np.power(np.dot(W,H),beta-1), H.T)\n",
    "        mur = np.divide(mur_up, mur_down)\n",
    "        W = np.multiply(W, mur)\n",
    "        \n",
    "        # To avoid underflow, we clip H and W to 10^-150\n",
    "        # so that a multiplication between them is not higher\n",
    "        # than 10^-307 (smallest exponent on this machine)\n",
    "        H = np.clip(H, 10**-150, None)\n",
    "        W = np.clip(W, 10**-150, None)\n",
    "        \n",
    "        bdiv = _beta_div(V,W,H,beta)\n",
    "        if verbose:\n",
    "            print(\"Iteration \"+str(it+1)+\" - Error : \" +str(bdiv))\n",
    "        # Check if approximation error relative decrease is below the desired threshold\n",
    "        rel_dec = ((error - bdiv) / init_error)\n",
    "        if verbose:\n",
    "            print(\"Iteration \"+str(it+1)+\" - Error Relative Decrease : \"+str(rel_dec))\n",
    "        if (rel_dec < tol) & (rel_dec > 0) :\n",
    "            break\n",
    "        error = bdiv\n",
    "            \n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.141s.\n",
      "Vectorizing...\n",
      "done in 0.271s.\n",
      "Initial error: 993340.6257554608\n",
      "Iteration 1 - Error : 28006.792799539184\n",
      "Iteration 1 - Error Relative Decrease : 0.9718054491345914\n",
      "Iteration 2 - Error : 27889.39777531558\n",
      "Iteration 2 - Error Relative Decrease : 0.00011818204267475769\n",
      "Iteration 3 - Error : 27804.75570314812\n",
      "Iteration 3 - Error Relative Decrease : 8.520951421179389e-05\n"
     ]
    }
   ],
   "source": [
    "features, names = featuresToVector()\n",
    "W, H = custom_NMF(features.toarray(), k=2, beta = 1, tol = 0.0001, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm above implemented seems to satisfy all the requirement listed. However, to avoid underflow a clipping procedure was applied at each iteration, reducing consequently the algorithm's precision. A more sofisticated rescaling technique should have been applied instead. Nonetheless, the algorithm produces satisfying results and converges in few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
