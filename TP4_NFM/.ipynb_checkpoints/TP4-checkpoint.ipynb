{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NONNEGATIVE MATRIX FACTORISATION FOR TOPIC EXTRACTION\n",
    "## - TOPIC EXTRACTION FROM DOCUMENTS -\n",
    "\n",
    "The goal is to study the use of nonnegative matrix factorisation (NMF) for topic extraction from a dataset of text documents. The rationale is to interpret each extracted NMF component as being associated with a specific topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# Lars Buitinck\n",
    "# Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\\\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 161.381s.\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it. We use a few\n",
    "# heuristics to filter out useless terms early on: the posts are\n",
    "# stripped of headers, footers and quoted replies, and common\n",
    "# English words, words occurring in only one document or in at\n",
    "# least 95% of the documents are removed.\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features...\n",
      "done in 1.289s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "max_features=n_features,\n",
    "stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 5.391s.\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "\"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test and comment on the effect of varying the initialisation, especially using random nonnegative values as initial guesses (for W and H coefficients, using the notations introduced during the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.853s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just like don people time know good make way use right say ve really want government ll new did going\n",
      "Topic #1: christian bible true christians faith jesus religion people does christ belief church life truth read reading believe statement atheism claim\n",
      "Topic #2: god jesus sin heaven lord christ believe mary does bible love human knowledge life marriage faith say children atheism knows\n",
      "Topic #3: think don people win extra just early sold sex need actually happen means pretty toronto wasn agree david statement mike\n",
      "Topic #4: drive drives hard disk software floppy card mac 00 computer scsi controller power apple mb pc sale rom monitor internal\n",
      "Topic #5: thanks know does mail advance hi info interested email anybody looking card help like appreciated list send information video need\n",
      "Topic #6: windows file dos files program using use window problem os help running drivers application pc ms ftp version screen available\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: car cars tires miles new engine insurance 00 price condition oil speed 000 good power brake year models bought area\n",
      "Topic #9: edu soon com send university internet mit ftp mail cc article pub information hope email mac blood home contact program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "\"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1, \n",
    "          init='random', alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare and comment on the difference between the results obtained with `2 cost compared to the generalised Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test and comment on the results obtained using a simpler term-frequency representation as input (as opposed to the TF-IDF representation considered in the code above) when considering the Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CUSTOM NMF IMPLEMENTATION -\n",
    "Implement the multiplicative update rules (derived from the majorisation-minimisation approach) for NMF estimation with β divergences, including the case β = 1 (generalised Kullback-Liebler divergence). Ensure that:\n",
    "- You can easily choose a custom initialisation for the W and H matrices;\n",
    "- You can set a custom number of iteration;\n",
    "- You can monitor the behaviour of the loss function across the iterations and that it is readily decreasing.\n",
    "\n",
    "Compare your implementation with the one offered by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfm(V=None, W=None, H=None, k=2, max_iter=1000, alpha=0.0002, beta=0.02):\n",
    "\n",
    "    # If V is not passed, we can't really do anything\n",
    "    if V is None:\n",
    "        raise ValueError('Please define matrix V')\n",
    "            \n",
    "    # If W is not passed, initialize as random\n",
    "    if W is None:\n",
    "        W = np.random.rand(V.shape[0], k)\n",
    "\n",
    "    # If H is not passed, initialize as random\n",
    "    if H is None:\n",
    "        H = np.random.rand(k, V.shape[1])\n",
    "        \n",
    "    # V and W must have same number of rows\n",
    "    if V.shape[0] != W.shape[0]:\n",
    "        raise ValueError('V and W must have same number of rows')\n",
    "        \n",
    "    # V and H must have same number of columns\n",
    "    if H.shape[1] != H.shape[1]:\n",
    "        raise ValueError('V and H must have same number of columns')\n",
    "    \n",
    "    # W and H must have respectively columns and rows equal to k\n",
    "    if (W.shape[1] != k) | (H.shape[0] != k) :\n",
    "        raise ValueError('W and H must have respectively columns and rows equal to k.')1\n",
    "    \n",
    "    r = \n",
    "    for step in range(max_iter):\n",
    "        for i in range(V.shape[0]):\n",
    "            for j in range():\n",
    "                if V[i][j] > 0:\n",
    "                    eij = V[i][j] - np.dot(W[i,:], H[:,j])\n",
    "                    for k in range(k):\n",
    "                        W[i][k] = W[i][k] + alpha * (2 * eij * H[k][j] - beta * W[i][k])\n",
    "                        H[k][j] = H[k][j] + alpha * (2 * eij * W[i][k] - beta * H[k][j])\n",
    "        eV = np.dot(W, H)\n",
    "        e = 0\n",
    "        for i in range(len(V)):\n",
    "            for j in range(len(V[i])):\n",
    "                if V[i][j] > 0:\n",
    "                    e = e + pow(V[i][j] - np.dot(W[i,:], H[:,j]), 2)\n",
    "                    for k in range(k):\n",
    "                        e = e + (beta/2) * ( pow(W[i][k], 2) + pow(H[k][j], 2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return W, H.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29110172, 0.71267274],\n",
       "       [0.7792501 , 0.10689938],\n",
       "       [0.09858641, 0.79002254],\n",
       "       [0.50845238, 0.48445975],\n",
       "       [0.88890585, 0.98645533]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = [\n",
    "         [5,3,0,1],\n",
    "         [4,0,0,1],\n",
    "         [1,1,0,5],\n",
    "         [1,0,0,4],\n",
    "         [0,1,5,4],\n",
    "]\n",
    "R = np.array(R)\n",
    "\n",
    "N = len(R)\n",
    "M = len(R[0])\n",
    "K = 2\n",
    "\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(K, M)\n",
    "\n",
    "nP, nQ = nfm(V=R, W=P, H=Q, k=K)\n",
    "nP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from numpy import dot\n",
    "\n",
    "def nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    print 'Starting NMF decomposition with {} latent features and {} iterations.'.format(latent_features, max_iter)\n",
    "    X = X.toarray()  # I am passing in a scipy sparse matrix\n",
    "\n",
    "    # mask\n",
    "    mask = np.sign(X)\n",
    "\n",
    "    # initial matrices. A is random [0,1] and Y is A\\X.\n",
    "    rows, columns = X.shape\n",
    "    A = np.random.rand(rows, latent_features)\n",
    "    A = np.maximum(A, eps)\n",
    "\n",
    "    Y = linalg.lstsq(A, X)[0]\n",
    "    Y = np.maximum(Y, eps)\n",
    "\n",
    "    masked_X = mask * X\n",
    "    X_est_prev = dot(A, Y)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # ===== updates =====\n",
    "        # Matlab: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y'));\n",
    "        top = dot(masked_X, Y.T)\n",
    "        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n",
    "        A *= top / bottom\n",
    "\n",
    "        A = np.maximum(A, eps)\n",
    "        # print 'A',  np.round(A, 2)\n",
    "\n",
    "        # Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y))));\n",
    "        top = dot(A.T, masked_X)\n",
    "        bottom = dot(A.T, mask * dot(A, Y)) + eps\n",
    "        Y *= top / bottom\n",
    "        Y = np.maximum(Y, eps)\n",
    "        # print 'Y', np.round(Y, 2)\n",
    "\n",
    "\n",
    "        # ==== evaluation ====\n",
    "        if i % 5 == 0 or i == 1 or i == max_iter:\n",
    "            print 'Iteration {}:'.format(i),\n",
    "            X_est = dot(A, Y)\n",
    "            err = mask * (X_est_prev - X_est)\n",
    "            fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "            X_est_prev = X_est\n",
    "\n",
    "            curRes = linalg.norm(mask * (X - X_est), ord='fro')\n",
    "            print 'fit residual', np.round(fit_residual, 4),\n",
    "            print 'total residual', np.round(curRes, 4)\n",
    "            if curRes < error_limit or fit_residual < fit_error_limit:\n",
    "                break\n",
    "\n",
    "return A, Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
