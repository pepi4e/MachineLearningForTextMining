{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA214 Machine Learning For Text Mining - Lab 2\n",
    "## Sentiment Analysis\n",
    "\n",
    "## PART ONE - Classifier Implementation \n",
    "The goal of this TP is to implement a Naive Bayes classifier to perform sentiment analysis on movies, trying to predict if they will have a positive or negative review. Our dataset is composed by 2000 review documents, each one labelled either as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sentimentanalysis import NB\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete the count_words function that will count the number of occurrences of each distinct word in a list of string and return vocabulary (the python dictionary. The dictionary keys are the different words and the values are their number of occurences).\n",
    "\n",
    "In the *count_words* function implementation, there are fundamentally two choices: get immediately all distinct words present in the collection of texts and then update their count, or start with an empty vocabulary and update it each time a new word is encountered. The second approach was followed.\n",
    "The function, in fact, starts with an empty vocabulary and starts counting words: if the current word is already known, it updates the count of this word in the current text; if the word is not already known, it appends it at the end of current's text count array and adds a new rule to the vocabulary, corresponding to the same index. In the end, zero padding has to be applied to arrays to return a NxN matrix.\n",
    "With respect to the returned **vocabulary** dict, I found a little contrast between this question's specifications on the subject and the comments at the beginning of the method provided. The specifications at the beginning of the method were followed, so vocabulary is a dictionary containing known words as key and as value the index that corresponds to the position of word count in the counts array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, stop_words=None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    words = set()\n",
    "    for text in texts:\n",
    "        words = words.union(set(text.split()))\n",
    "    n_features = len(words)\n",
    "    n_samples = len(texts)\n",
    "    vocabulary = dict(zip(words), range(n_features))\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    for k, text in enumerate(texts):\n",
    "        counts[k][vocabulary[w]] += 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    vocabulary = {}    \n",
    "    counts = []\n",
    "    j=0\n",
    "    # Remove reduntant stop words\n",
    "    sw = list(set(stop_words)) if stop_words is not None else []\n",
    "    # Counts words in each text. \n",
    "    # New words encountered are appended to the end of the array\n",
    "    for text in texts:\n",
    "        # Split text in words        \n",
    "        words = re.split(' |; |, |\\n', text)\n",
    "        # Initialize text vocabulary to known words\n",
    "        single_count = [0]*len(vocabulary)\n",
    "        for word in words:\n",
    "            # Check for stop words\n",
    "            if word not in sw:\n",
    "                if word in vocabulary:\n",
    "                    single_count[vocabulary[word]] += 1\n",
    "                else:\n",
    "                    # Add new word both in vocabulary and in current result\n",
    "                    single_count.append(1)\n",
    "                    vocabulary[word] = j\n",
    "                    j = j+1\n",
    "        counts.append(single_count)\n",
    "    \n",
    "    # Pad arrays with zeros (first arrays don't contain all words)\n",
    "    countsout = np.zeros((len(texts), len(vocabulary.keys())), dtype=np.int32)\n",
    "    i=0\n",
    "    for c in counts:\n",
    "        countsout[i, :len(c)] = c\n",
    "        i = i+1\n",
    "    return vocabulary, countsout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how positive and negative classes have been assigned to movie reviews (see poldata.README.2.0 file)\n",
    "According to what is written in the file, reviews have a positive label if they have a rating higher or equal to 3.5 stars out of 5 or 4 out 5, and a grade not lower than B. Reviews have a negative review if their rating is lower than 2/5 stars, or 1.5/4 stars, or a grade equal or lower than C.\n",
    "\n",
    "### 3. Complete the NB class to implement the Naive Bayes classifier\n",
    "Complete the NB class means to carry out the implementation of *fit* and *predict* methods.\n",
    "\n",
    "The *fit* method computes the probabilities for each class (which is the **prior** probability) simply as Nc = number of texts having class c / N = all texts and the probabilities for each word w conditioned to class c (which the **conditional** probability), representing the probability of having word w in a text knowing that its class is c.\n",
    "\n",
    "The *predict* method carries out the predicting task by computing a score for each class c: the class with the higher score will then be choosen as the prediction. The score of each class is initialized to the prior probability of that class, and then incremented by the logarithm of the conditional probability for that class of each word present in the text to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_classes = None\n",
    "        self.classes = {}\n",
    "        self.prior = {}\n",
    "        self.condprob = {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Compute number of docs\n",
    "        N = len(y)\n",
    "        # Compute classes\n",
    "        self.classes = np.unique(y)\n",
    "        self.num_classes = len(self.classes)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # Compute prior probability\n",
    "            # Take only wordcounts of data with class c\n",
    "            useful_data = np.array([ X[idx, :] for idx, val in enumerate(y) if val == c ])\n",
    "            Nc = len(useful_data)\n",
    "            self.prior[c] = Nc/N\n",
    "            # Compute conditional probability\n",
    "            totcount = {}\n",
    "            # Sum columns\n",
    "            totcount[c] = np.array([ sum(x) for x in zip(*useful_data) ])\n",
    "            # Compute Laplacian\n",
    "            lapl = np.sum(totcount[c]) + len(totcount[c])\n",
    "            # Smooth\n",
    "            self.condprob[c] = (totcount[c]+1)/lapl\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sc = np.zeros((X.shape[0], self.num_classes))\n",
    "        for c in self.classes:\n",
    "            # Initialize scores sc to prior probability\n",
    "            sc[:, int(c)] = np.log(self.prior[c])\n",
    "            # Compute log value\n",
    "            cp = np.log(self.condprob[c])\n",
    "            for x in range(X.shape[0]-1):\n",
    "                # Add log value if word is present in the text\n",
    "                sc[x, int(c)] = sum([ val for idx, val in enumerate(cp) if X[x][idx] != 0])\n",
    "        return [ np.argmax(sc[x]) for x in range(X.shape[0]) ]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the performance of your classifier in cross-validation 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data...\n",
      "Loaded stop words...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb2', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb2', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print('Loaded data...')\n",
    "\n",
    "# Split processed data in train and test data\n",
    "# Random permutation to split the data randomly\n",
    "indices = np.random.permutation(len(texts))\n",
    "size = int(len(texts)/2)\n",
    "X = texts\n",
    "X_train = [ X[i] for i in indices[:-size] ]\n",
    "y_train = y[indices[:-size]]\n",
    "X_test = [ X[i] for i in indices[-size:] ]\n",
    "y_test = y[indices[-size:]]\n",
    "\n",
    "# Load stop words\n",
    "stop_words = open('./data/english.stop').read().split()\n",
    "print('Loaded stop words...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stop words...\n",
      "Cross Validation 5-fold score: 0.6776190476190477 mean 0.05735278897863748 std \n"
     ]
    }
   ],
   "source": [
    "vocabulary, X_proc = count_words(X, stop_words)\n",
    "\n",
    "nb = NB()\n",
    "score = cross_val_score(nb, X_proc, y, cv=5)\n",
    "print('With stop words...')\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Change the count_words function to ignore the “stop words” in the file data/english.stop. Are the performances improved ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stop words...\n",
      "Cross Validation 5-fold score: 0.6735714285714286 mean 0.07635176380893041 std \n"
     ]
    }
   ],
   "source": [
    "vocabulary, X_proc = count_words(X)\n",
    "\n",
    "nb = NB()\n",
    "score = cross_val_score(nb, X_proc, y, cv=5)\n",
    "print('Without stop words...')\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that removing stop words helps - even if by little - in increasing accuracy. \n",
    "In both cases, an accuracy higher than 0.8 is a very satisfying result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART TWO - Scikit-Learn Use\n",
    "### 1. Compare your implementation with scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Count Vectorizer (counting words) ---\n",
      "Cross Validation 5-fold score: 0.7030952380952381 mean 0.07505704557670086 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Simple Count Vectorizer (counting words) ---')\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count Vectorizer with char analyzer ---\n",
      "Cross Validation 5-fold score: 0.5585714285714285 mean 0.05386112149002532 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Count Vectorizer with char analyzer ---')\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Count Vectorizer with ngrams ---\n",
      "Cross Validation 5-fold score: 0.5488095238095239 mean 0.05317045707598285 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Simple Count Vectorizer with ngrams ---')\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char_wb')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with CountVectorizer options seems not to help accuracy results, which are at their highest when the Count Vectorizer is used with default settings. This happens probably due to the fact that text data is not always well defined, as seen while testing: lots of sentences contain formatting / web characters (i.e. '\\n'), which may lead to unconsistent representations.\n",
    "\n",
    "What is impressive is that our implementation of the Naive Bayes classifier was able to obtain results almost as good as the scikit-learn implementation! Obviously, in terms of time (and, consequently, of efficiency) the scikit-learn implementation vastly outperforms ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test another classification method scikitlearn (ex : LinearSVC, LogisticRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6732673267326733\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Score: ' + str(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use NLTK library in order to process a stemming. You will used the class SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming - My Naive Bayes Classifier ---\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'f' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b8ba5dc36c0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--- Stemming - My Naive Bayes Classifier ---'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cross Validation 5-fold score: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' mean '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' std '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-b8ba5dc36c0a>\u001b[0m in \u001b[0;36mstemText\u001b[1;34m(texts, stop_words, use_pos_tag)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0muse_pos_tag\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JJ'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'f' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def stemText(texts, stop_words=None, use_pos_tag=True):\n",
    "    res = []\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    for text in texts:\n",
    "        res.append([])\n",
    "        for t in re.split(' |; |, |\\n', text):\n",
    "            stem = stemmer.stem(t)            \n",
    "            if t not in stop_words:\n",
    "                if use_pos_tag is True:\n",
    "                    f = (pos_tag([stem])[0][1])\n",
    "                    if f in ['NN', 'VB', 'RB', 'JJ']):\n",
    "                        print(t)\n",
    "                        print(stem)\n",
    "                        print(f)\n",
    "                        res[-1].append(f)\n",
    "                elif use_pos_tag is False:\n",
    "                    res[-1].append(stem)\n",
    "    return [ \" \".join(r) for r in res ]\n",
    "\n",
    "print('--- Stemming - My Naive Bayes Classifier ---')\n",
    "score = cross_val_score(nb, stemText(X,stop_words), y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')\n",
    "\n",
    "print('--- Stemming - LinearSVC ---')\n",
    "score = cross_val_score(pipe, stemText(X,stop_words), y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ciao', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"ciao\"\n",
    "pos_tag([a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter words by grammatical category (POS : Part Of Speech) and keep only nouns, verbs, adverbs and adjectives for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
