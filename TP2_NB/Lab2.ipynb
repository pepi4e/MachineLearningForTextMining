{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA214 Machine Learning For Text Mining - Lab 2\n",
    "## Sentiment Analysis\n",
    "\n",
    "## PART ONE - Classifier Implementation \n",
    "The goal of this TP is to implement a Naive Bayes classifier to perform sentiment analysis on movies, trying to predict if they will have a positive or negative review. Our dataset is composed by 2000 review documents, each one labelled either as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sentimentanalysis import NB\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete the count_words function that will count the number of occurrences of each distinct word in a list of string and return vocabulary (the python dictionary. The dictionary keys are the different words and the values are their number of occurences).\n",
    "\n",
    "In the *count_words* function implementation, there are fundamentally two choices: get immediately all distinct words present in the collection of texts and then update their count, or start with an empty vocabulary and update it each time a new word is encountered. The second approach was followed.\n",
    "The function, in fact, starts with an empty vocabulary and starts counting words: if the current word is already known, it updates the count of this word in the current text; if the word is not already known, it appends it at the end of current's text count array and adds a new rule to the vocabulary, corresponding to the same index. In the end, zero padding has to be applied to arrays to return a NxN matrix.\n",
    "With respect to the returned **vocabulary** dict, I found a little contrast between this question's specifications on the subject and the comments at the beginning of the method provided. The specifications at the beginning of the method were followed, so vocabulary is a dictionary containing known words as key and as value the index that corresponds to the position of word count in the counts array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, stop_words=None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    words = set()\n",
    "    for text in texts:\n",
    "        words = words.union(set(text.split()))\n",
    "    n_features = len(words)\n",
    "    n_samples = len(texts)\n",
    "    vocabulary = dict(zip(words), range(n_features))\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    for k, text in enumerate(texts):\n",
    "        counts[k][vocabulary[w]] += 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    vocabulary = {}    \n",
    "    counts = []\n",
    "    j=0\n",
    "    # Remove reduntant stop words\n",
    "    sw = list(set(stop_words)) if stop_words is not None else []\n",
    "    # Counts words in each text. \n",
    "    # New words encountered are appended to the end of the array\n",
    "    for text in texts:\n",
    "        # Split text in words        \n",
    "        words = re.split(' |; |, |\\n', text)\n",
    "        # Initialize text vocabulary to known words\n",
    "        single_count = [0]*len(vocabulary)\n",
    "        for word in words:\n",
    "            # Check for stop words\n",
    "            if word not in sw:\n",
    "                if word in vocabulary:\n",
    "                    single_count[vocabulary[word]] += 1\n",
    "                else:\n",
    "                    # Add new word both in vocabulary and in current result\n",
    "                    single_count.append(1)\n",
    "                    vocabulary[word] = j\n",
    "                    j = j+1\n",
    "        counts.append(single_count)\n",
    "    \n",
    "    # Pad arrays with zeros (first arrays don't contain all words)\n",
    "    countsout = np.zeros((len(texts), len(vocabulary.keys())), dtype=np.int32)\n",
    "    i=0\n",
    "    for c in counts:\n",
    "        countsout[i, :len(c)] = c\n",
    "        i = i+1\n",
    "    return vocabulary, countsout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how positive and negative classes have been assigned to movie reviews (see poldata.README.2.0 file)\n",
    "According to what is written in the file, reviews have a positive label if they have a rating higher or equal to 3.5 stars out of 5 or 4 out 5, and a grade not lower than B. Reviews have a negative review if their rating is lower than 2/5 stars, or 1.5/4 stars, or a grade equal or lower than C.\n",
    "\n",
    "### 3. Complete the NB class to implement the Naive Bayes classifier\n",
    "Complete the NB class means to carry out the implementation of *fit* and *predict* methods.\n",
    "\n",
    "The *fit* method computes the probabilities for each class (which is the **prior** probability) simply as Nc = number of texts having class c / N = all texts and the probabilities for each word w conditioned to class c (which the **conditional** probability), representing the probability of having word w in a text knowing that its class is c.\n",
    "\n",
    "The *predict* method carries out the predicting task by computing a score for each class c: the class with the higher score will then be choosen as the prediction. The score of each class is initialized to the prior probability of that class, and then incremented by the logarithm of the conditional probability for that class of each word present in the text to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_classes = None\n",
    "        self.classes = {}\n",
    "        self.prior = {}\n",
    "        self.condprob = {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Compute number of docs\n",
    "        N = len(y)\n",
    "        # Compute classes\n",
    "        self.classes = np.unique(y)\n",
    "        self.num_classes = len(self.classes)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # Compute prior probability\n",
    "            # Take only wordcounts of data with class c\n",
    "            useful_data = np.array([ X[idx, :] for idx, val in enumerate(y) if val == c ])\n",
    "            Nc = len(useful_data)\n",
    "            self.prior[c] = Nc/N\n",
    "            # Compute conditional probability\n",
    "            totcount = {}\n",
    "            # Sum columns\n",
    "            totcount[c] = np.array([ sum(x) for x in zip(*useful_data) ])\n",
    "            # Compute Laplacian\n",
    "            lapl = np.sum(totcount[c]) + len(totcount[c])\n",
    "            # Smooth\n",
    "            self.condprob[c] = (totcount[c]+1)/lapl\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sc = np.zeros((X.shape[0], self.num_classes))\n",
    "        for c in self.classes:\n",
    "            # Initialize scores sc to prior probability\n",
    "            sc[:, int(c)] = np.log(self.prior[c])\n",
    "            # Compute log value\n",
    "            cp = np.log(self.condprob[c])\n",
    "            for x in range(X.shape[0]-1):\n",
    "                # Add log value if word is present in the text\n",
    "                sc[x, int(c)] = sum([ val for idx, val in enumerate(cp) if X[x][idx] != 0])\n",
    "        return [ np.argmax(sc[x]) for x in range(X.shape[0]) ]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the performance of your classifier in cross-validation 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data...\n",
      "Loaded stop words...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print('Loaded data...')\n",
    "\n",
    "# Split processed data in train and test data\n",
    "# Random permutation to split the data randomly\n",
    "indices = np.random.permutation(len(texts))\n",
    "size = int(len(texts)/2)\n",
    "X = texts\n",
    "X_train = [ X[i] for i in indices[:-size] ]\n",
    "y_train = y[indices[:-size]]\n",
    "X_test = [ X[i] for i in indices[-size:] ]\n",
    "y_test = y[indices[-size:]]\n",
    "\n",
    "# Load stop words\n",
    "stop_words = open('./data/english.stop').read().split()\n",
    "print('Loaded stop words...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stop words...\n",
      "Cross Validation 5-fold score: 0.8215 mean 0.015049916943292404 std \n"
     ]
    }
   ],
   "source": [
    "vocabulary, X_proc = count_words(X, stop_words)\n",
    "\n",
    "nb = NB()\n",
    "score = cross_val_score(nb, X_proc, y, cv=5)\n",
    "print('With stop words...')\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Change the count_words function to ignore the “stop words” in the file data/english.stop. Are the performances improved ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stop words...\n",
      "Cross Validation 5-fold score: 0.8275 mean 0.008366600265340763 std \n"
     ]
    }
   ],
   "source": [
    "vocabulary, X_proc = count_words(X)\n",
    "\n",
    "nb = NB()\n",
    "score = cross_val_score(nb, X_proc, y, cv=5)\n",
    "print('Without stop words...')\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that not using stop words helps - even if by little - in increasing accuracy. \n",
    "In both cases, an accuracy higher than 0.8 is a very satisfying result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART TWO - Scikit-Learn Use\n",
    "### 1. Compare your implementation with scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Count Vectorizer (counting words) ---\n",
      "Cross Validation 5-fold score: 0.812 mean 0.012884098726725092 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Simple Count Vectorizer (counting words) ---')\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count Vectorizer with char analyzer ---\n",
      "Cross Validation 5-fold score: 0.6094999999999999 mean 0.018261982367749664 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Count Vectorizer with char analyzer ---')\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Count Vectorizer with ngrams ---\n",
      "Cross Validation 5-fold score: 0.6115 mean 0.014456832294800962 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Simple Count Vectorizer with ngrams ---')\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char_wb')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with CountVectorizer options seems not to help accuracy results, which are at their highest when the Count Vectorizer is used with default settings. This happens probably due to the fact that text data is not always well defined, as seen while testing: lots of sentences contain formatting / web characters (i.e. '\\n'), which may lead to unconsistent representations with characters and ngrams.\n",
    "\n",
    "What is impressive is that our implementation of the Naive Bayes classifier was able to obtain better results (0.82 vs 0.81) than the scikit-learn implementation!\n",
    "\n",
    "Obviously, in terms of time (and, consequently, of efficiency) the scikit-learn implementation vastly outperforms ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test another classification method scikitlearn (ex : LinearSVC, LogisticRegression).\n",
    "The chosen classifier is a LinearSVC, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 5-fold score: 0.833 mean 0.01623268308074792 std \n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "score = cross_val_score(pipe, X, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all the classifier used/implemented in this lab, the LinearSVC is the one with the best accuracy, which is better than \"my\" Naive Bayes classifier's one and also than the scikit MultinomialNB's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use NLTK library in order to process a stemming. You will used the class SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemText(texts, stop_words=None, use_pos_tag=True):\n",
    "    res = []\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    for text in texts:\n",
    "        res.append([])\n",
    "        for t in re.split(' |; |, |\\\\n', text):\n",
    "            stem = stemmer.stem(t) \n",
    "            if (t not in stop_words) & (len(t) > 0):\n",
    "                if use_pos_tag is True:\n",
    "                    f = (pos_tag([stem])[0][1])\n",
    "                    if f in ['NN', 'VB', 'RB', 'JJ']:\n",
    "                        res[-1].append(stem)\n",
    "                elif use_pos_tag is False:\n",
    "                    res[-1].append(stem)\n",
    "    return [ \" \".join(r) for r in res ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming - My Naive Bayes Classifier ---\n",
      "Cross Validation 5-fold score: 0.8220000000000001 mean 0.013546217184144048 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Stemming - My Naive Bayes Classifier ---')\n",
    "vocabulary, X_proc = count_words(stemText(X,stop_words, use_pos_tag=False))\n",
    "score = cross_val_score(nb, X_proc, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming - LinearSVC ---\n",
      "Cross Validation 5-fold score: 0.8164999999999999 mean 0.00902773504263389 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Stemming - LinearSVC ---')\n",
    "score = cross_val_score(pipe, stemText(X,stop_words, use_pos_tag=False), y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter words by grammatical category (POS : Part Of Speech) and keep only nouns, verbs, adverbs and adjectives for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming and POS Filtering - My Naive Bayes Classifier ---\n",
      "Cross Validation 5-fold score: 0.8210000000000001 mean 0.009027735042633867 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Stemming and POS Filtering - My Naive Bayes Classifier ---')\n",
    "vocabulary, X_proc = count_words(stemText(X,stop_words, use_pos_tag=True))\n",
    "score = cross_val_score(nb, X_proc, y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming and POS Filtering - LinearSVC ---\n",
      "Cross Validation 5-fold score: 0.812 mean 0.016462077633154326 std \n"
     ]
    }
   ],
   "source": [
    "print('--- Stemming and POS Filtering - LinearSVC ---')\n",
    "score = cross_val_score(pipe, stemText(X,stop_words, use_pos_tag=True), y, cv=5)\n",
    "print('Cross Validation 5-fold score: ' + str(score.mean()) + ' mean ' + str(score.std()) + ' std ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using stemming seems to not improve our classifier performances. Also filtering words by their POS doesn't increase accuracy.\n",
    "\n",
    "About the implementation, it has to be said that it is higly inefficient, as confirmed by extremely long run times: the *stemText* method stems and filters words one by one and, in the overall process, we check two times for stop words (both in *stemText* and *count_words* methods)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
