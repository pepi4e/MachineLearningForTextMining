{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA214 Machine Learning For Text Mining - Lab 2\n",
    "## Sentiment Analysis\n",
    "\n",
    "## PART ONE - Classifier Implementation \n",
    "The goal of this TP is to implement a Naive Bayes classifier to perform sentiment analysis on movies, trying to predict if they will have a positive or negative review. Our dataset is composed by 2000 review documents, each one labelled either as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete the count_words function that will count the number of occurrences of each distinct word in a list of string and return vocabulary (the python dictionary. The dictionary keys are the different words and the values are their number of occurences).\n",
    "\n",
    "In the *count_words* function implementation, there are fundamentally two choices: get immediately all distinct words present in the collection of texts and then update their count, or start with an empty vocabulary and update it each time a new word is encountered. The second approach was followed.\n",
    "The function, in fact, starts with an empty vocabulary and starts counting words: if the current word is already known, it updates the count of this word in the current text; if the word is not already known, it appends it at the end of current's text count array and adds a new rule to the vocabulary, corresponding to the same index. In the end, zero padding has to be applied to arrays to return a NxN matrix.\n",
    "With respect to the returned **vocabulary** dict, I found a little contrast between this question's specifications on the subject and the comments at the beginning of the method provided. The specifications at the beginning of the method were followed, so vocabulary is a dictionary containing known words as key and as value the index that corresponds to the position of word count in the counts array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how positive and negative classes have been assigned to movie reviews (see poldata.README.2.0 file)\n",
    "According to what is written in the file, reviews have a positive label if they have a rating higher or equal to 3.5 stars out of 5 or 4 out 5, and a grade not lower than B. Reviews have a negative review if their rating is lower than 2/5 stars, or 1.5/4 stars, or a grade equal or lower than C.\n",
    "\n",
    "### 3. Complete the NB class to implement the Naive Bayes classifier\n",
    "Complete the NB class means to carry out the implementation of *fit* and *predict* methods.\n",
    "\n",
    "The *fit* method computes the probabilities for each class (which is the **prior** probability) simply as Nc = number of texts having class c / N = all texts and the probabilities for each word w conditioned to class c (which the **conditional** probability), representing the probability of having word w in a text knowing that its class is c.\n",
    "\n",
    "The *predict* method carries out the predicting task by computing a score for each class c: the class with the higher score will then be choosen as the prediction. The score of each class is initialized to the prior probability of that class, and then incremented by the logarithm of the conditional probability for that class of each word present in the text to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the performance of your classifier in cross-validation 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stop words...\n",
      "Cross Validation 5-fold score: 0.8215 mean 0.015049916943292404 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Change the count_words function to ignore the “stop words” in the file data/english.stop. Are the performances improved ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stop words...\n",
      "Cross Validation 5-fold score: 0.8275 mean 0.008366600265340763 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that not using stop words helps - even if by little - in increasing accuracy. \n",
    "In both cases, an accuracy higher than 0.8 is a very satisfying result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART TWO - Scikit-Learn Use\n",
    "### 1. Compare your implementation with scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Count Vectorizer (counting words) ---\n",
      "Cross Validation 5-fold score: 0.812 mean 0.012884098726725092 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count Vectorizer with char analyzer ---\n",
      "Cross Validation 5-fold score: 0.6094999999999999 mean 0.018261982367749664 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Count Vectorizer with ngrams ---\n",
      "Cross Validation 5-fold score: 0.6115 mean 0.014456832294800962 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with CountVectorizer options seems not to help accuracy results, which are at their highest when the Count Vectorizer is used with default settings. This happens probably due to the fact that text data is not always well defined, as seen while testing: lots of sentences contain formatting / web characters (i.e. '\\n'), which may lead to unconsistent representations with characters and ngrams.\n",
    "\n",
    "What is impressive is that our implementation of the Naive Bayes classifier was able to obtain better results (0.82 vs 0.81) than the scikit-learn implementation!\n",
    "\n",
    "Obviously, in terms of time (and, consequently, of efficiency) the scikit-learn implementation vastly outperforms ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test another classification method scikitlearn (ex : LinearSVC, LogisticRegression).\n",
    "The chosen classifier is a LinearSVC, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 5-fold score: 0.833 mean 0.01623268308074792 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all the classifier used/implemented in this lab, the LinearSVC is the one with the best accuracy, which is better than \"my\" Naive Bayes classifier's one and also than the scikit MultinomialNB's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use NLTK library in order to process a stemming. You will used the class SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming - My Naive Bayes Classifier ---\n",
      "Cross Validation 5-fold score: 0.8220000000000001 mean 0.013546217184144048 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming - LinearSVC ---\n",
      "Cross Validation 5-fold score: 0.8164999999999999 mean 0.00902773504263389 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter words by grammatical category (POS : Part Of Speech) and keep only nouns, verbs, adverbs and adjectives for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming and POS Filtering - My Naive Bayes Classifier ---\n",
      "Cross Validation 5-fold score: 0.8210000000000001 mean 0.009027735042633867 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming and POS Filtering - LinearSVC ---\n",
      "Cross Validation 5-fold score: 0.812 mean 0.016462077633154326 std \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using stemming seems to not improve our classifier performances. Also filtering words by their POS doesn't increase accuracy.\n",
    "\n",
    "About the implementation, it has to be said that it is higly inefficient, as confirmed by extremely long run times: the *stemText* method stems and filters words one by one and, in the overall process, we check two times for stop words (both in *stemText* and *count_words* methods)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
