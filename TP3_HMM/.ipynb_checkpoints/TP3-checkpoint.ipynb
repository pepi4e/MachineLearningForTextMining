{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA214 Machine Learning For Text Mining\n",
    "# Text segmentation using Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : automatic segmentation of mails\n",
    "This Lab aims to build an email segmentation tool, dedicated to separate the email header from its body. It is proposed to perform this task by learning a HMM with two states, one (state 1) for the header, the other (state 2) for the body. In this model, it is assumed that each mail actually contains a header : the decoding necessarily begins in the state 1.\n",
    "\n",
    "## 1. Give the value of the π vector of the initial probabilities\n",
    "It is said above that our HMM has necessarily an initial state of 1. So, the π vector of the initial probabilities will have the following form:\n",
    "\n",
    "$$\\pi^{T} = (1, 0)$$\n",
    "\n",
    "Since the HMM will be in state 0 with probability 0 and in state 1 with probability 1, i.e. always.\n",
    "\n",
    "Knowing that each mail contains exactly one header and one body, each mail follows once the transition from 1 to 2. The transition matrix (A(i, j) = P(j|i)) estimated on a labeled small corpus has thus the following form :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99218078e-01, 7.81921964e-04],\n",
       "       [0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[0.999218078035812, 0.000781921964187974], [0, 1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is the probability to move from state 1 to state 2 ? What is the probability to remain in state 2 ? What is the lower/higher probability ? Try to explain why \n",
    "\n",
    "Probabilities of movements between states can be found in the transition matrix A, which was just given. The i index (row index) determines the starting state, while the j index (column index) determines the arriving state. Thus:\n",
    "\n",
    "- Probability to move from state 1 to state 2: A(1,2) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000781921964187974"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probability to remain in state 2: A(2,2) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability to remain in state 2 is actually the highest obtainable probability (=1), and this makes sense in our use case: once our email is in the state corresponding to the body, we can't go back anymore to the header state with more iterations, that correspond to reading more characters. In fact, an email's body goes from its beginning to end of an email, and in no case is followed by another header. As a consequence, when the number of iterations/observation n goes to infinity, our observed Markov Chain will forcely be in state 1, meaning that - as stated before - we will necessarily be in the email's body.\n",
    "\n",
    "On the contrary, the probability of moving from state 1 to state 2 is much lower because in average we have to read a significant number of characters - and, therefore, wait for several iterations - before being able to affirm that the email has also a body.\n",
    "\n",
    "For example, if we had built our model and the transition matrix A using some strange kind of emails that have 3 characters in the header and (in average) more than 2000 in the body, this probability would be much higher. Obviously, the creation of the HMM and thus our probabilty also depend on the observations, which influence directly the hidden process and indirectly the observed one: if I had trained it with headers containing only 'a' characters and bodies that never contain the 'a' character, this would surely impact the transition probability matrix.\n",
    "\n",
    "A mail is represented by a sequence of characters. Let N be the number of different characters. Each part of the mail is characterized by a discrete probability distribution on the characters P(c|s), with s = 1 or s = 2.\n",
    "\n",
    "## 3. What is the size of the corresponding matrix ?\n",
    "The size of the corrispondig matrix is $$ N \\times |s| $$\n",
    "Since we are encoding characters as ASCII values, which range from 1 to 256, the matrix will have a size of 256x2, namely 256 rows and 2 columns.\n",
    "\n",
    "## Material\n",
    "### Coding/decoding mails\n",
    "\n",
    "Emails are represented as ASCII character vectors.\n",
    "In dat.zip, mail.txt in a vector of numbers (one vector per line). that can be transformed into a vector of numbers (between 0 and 255) in a text;\n",
    "Files of the form dat /*. dat contain the already encoded versions of the corresponding mails. The list is in mail.lst.\n",
    "\n",
    "### Visualizing segmentation\n",
    "Finally, the utility segment.pl allows to visualize a segmentation produced by your segmenter in the form of the best path found by the Viterbi algorithm (in a vector of 1 and 2). It produces a file where the segmentation is visualized. \n",
    "\n",
    "To use it :\n",
    "\n",
    "perl segment.pl mail.txt path.dat\n",
    "\n",
    "### Distribution files\n",
    "For the first part of the Lab, we work with the distributions that are provided in the P.dat file. \n",
    "\n",
    "Each of the columns of this file contains the distribution of the probabilities of occurrence of each character of the ASCII codes respectively in the header and in the body. These distributions were learned on a small corpus labeled with 10 emails ; there are obvious differences, especially in areas where ASCII codes correspond to alphabetic characters, as you can see by viewing these distributions.\n",
    "\n",
    "#### To implement:\n",
    "\n",
    "All the work is to be done under Python.\n",
    "\n",
    "- implement the Viterbi algorithm. Concretely, it comes to coding a function which takes as argument a vector of observations and the parameters of the model, and returns a vector of states representing the most probable sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Viterbi Algorithm Implementation\n",
    "    \n",
    "    Inputs:\n",
    "        - obs: a 1-d nparray containing the observations. Its values must be lesser \n",
    "            or equal than the number of rows of the conditional probability matrix. \n",
    "        - states: a 1-d nparray containing the number of possible states. Its size\n",
    "            has to be equal to the number of columns of the conditional probability \n",
    "            matrix.\n",
    "        - trans: a 2-d nparray transition probability matrix containing the \n",
    "            transition probabilities for the observed process.\n",
    "        - start_prob: a 1-d nparray containing the starting probabilities for the\n",
    "            observed process. Its length has to be equal to the number of possible\n",
    "            states.\n",
    "        - cond_prob: also known as emission probability, it is a 2-d nparray \n",
    "            having a number of columns equal to the number of possible states.\n",
    "    \n",
    "    Returns:\n",
    "        - seq: a 1-d nparray containing, for each observation given in the input,\n",
    "            the corresponding state that represent the most probable sequence\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "def viterbiAlgorithm(obs, states, trans, start_prob, cond_prob):\n",
    "    # Rescaling function that avoids state probabilities going to zero\n",
    "    # Computes the order of magnitude of values in the array's last row \n",
    "    # and rescales them back to 10^-1\n",
    "    def rescale(state_probs, last_row):\n",
    "        # Compute order of magnitude\n",
    "        om = int(np.log10(np.max(state_probs[last_row,:])))\n",
    "        # Rescale\n",
    "#         print('Before rescaling ' + str(state_probs[last_row, :]) \n",
    "#                    + ' - OM ' + str(om) + ' Resc factor ' + str((om+1)))\n",
    "        state_probs[last_row, :] = state_probs[last_row, :]*(10**(-1*(om+1)))\n",
    "#         print('After rescaling ' + str(state_probs[last_row, :]))\n",
    "        return state_probs\n",
    "    \n",
    "    # State probabilities\n",
    "    state_probs = np.zeros((len(obs), len(states)))\n",
    "    # Sequence of states\n",
    "    path = {}\n",
    "    \n",
    "    for i in range(0,len(observations)):\n",
    "        \n",
    "        # First iteration\n",
    "        if i == 0:\n",
    "            # Initialize states probabilites \n",
    "            for state in range(0,len(states)):\n",
    "                state_probs[i][state] = start_prob[state]\\\n",
    "                                            * cond_prob[obs[i]][state]\n",
    "                path[state] = [states[state]]\n",
    "#             print('States probabilities: ' + str(state_probs[i]))\n",
    "        else:\n",
    "            # Update\n",
    "            tempPath = {}\n",
    "            probs = np.zeros((len(states), len(states)))            \n",
    "            for arr_state in range(0, len(states)):\n",
    "                (probs, state) = max([(state_probs[i-1][dep_state] \\\n",
    "                                       * trans[dep_state][arr_state]\\\n",
    "                                       * cond_prob[obs[i]][arr_state], \n",
    "                                      dep_state) \\\n",
    "                                      for dep_state in range(0, len(states))])\n",
    "#               print('Arrival ' + str(arr_state) + ' - Probabilities : '\n",
    "#               + str((probs, state)))\n",
    "                state_probs[i, arr_state] = probs\n",
    "                tempPath[arr_state] = path[state] + [states[arr_state]]\n",
    "#             print('Observation ' + str(i) + ' - Probabilities: ')\n",
    "#             print(str(probs))\n",
    "#             print('Max prob : ' + str(probs[r][c]) + ' - R, C : ' + str(r) + ', '\n",
    "#             + str(c) + ' - State: ' + str(states[c]))\n",
    "#             print('States probabilities: ' + str(state_probs[i]))\n",
    "            path =  tempPath   \n",
    "            state_probs = rescale(state_probs, i)\n",
    "    (finProb, state) = max([(state_probs[-1][s], s)\\\n",
    "                            for s in range(0, len(states))])\n",
    "    return path[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test it on some mails that are given in the dat directory (especially mail11.txt to mail30.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MAIL 11 STATES : \n",
      "State 1 observations 0 -> 2850\n",
      "State 2 observations 2851 -> 3474\n",
      "--- MAIL 12 STATES : \n",
      "State 1 observations 0 -> 2937\n",
      "State 2 observations 2938 -> 3992\n",
      "--- MAIL 13 STATES : \n",
      "State 1 observations 0 -> 2303\n",
      "State 2 observations 2304 -> 3327\n",
      "--- MAIL 14 STATES : \n",
      "State 1 observations 0 -> 4812\n",
      "State 2 observations 4813 -> 6575\n",
      "--- MAIL 15 STATES : \n",
      "State 1 observations 0 -> 2182\n",
      "State 2 observations 2183 -> 6807\n",
      "--- MAIL 16 STATES : \n",
      "State 1 observations 0 -> 1970\n",
      "State 2 observations 1971 -> 2626\n",
      "--- MAIL 17 STATES : \n",
      "State 1 observations 0 -> 2281\n",
      "State 2 observations 2282 -> 3424\n",
      "--- MAIL 18 STATES : \n",
      "State 1 observations 0 -> 2366\n",
      "State 2 observations 2367 -> 3076\n",
      "--- MAIL 19 STATES : \n",
      "State 1 observations 0 -> 2100\n",
      "State 2 observations 2101 -> 2619\n",
      "--- MAIL 20 STATES : \n",
      "State 1 observations 0 -> 1839\n",
      "State 2 observations 1840 -> 2433\n",
      "--- MAIL 21 STATES : \n",
      "State 1 observations 0 -> 2102\n",
      "State 2 observations 2103 -> 2663\n",
      "--- MAIL 22 STATES : \n",
      "State 1 observations 0 -> 2233\n",
      "State 2 observations 2234 -> 3642\n",
      "--- MAIL 23 STATES : \n",
      "State 1 observations 0 -> 2167\n",
      "State 2 observations 2168 -> 3749\n",
      "--- MAIL 24 STATES : \n",
      "State 1 observations 0 -> 2559\n",
      "State 2 observations 2560 -> 3700\n",
      "--- MAIL 25 STATES : \n",
      "State 1 observations 0 -> 2318\n",
      "State 2 observations 2319 -> 3237\n",
      "--- MAIL 26 STATES : \n",
      "State 1 observations 0 -> 2026\n",
      "State 2 observations 2027 -> 4466\n",
      "--- MAIL 27 STATES : \n",
      "State 1 observations 0 -> 1770\n",
      "State 2 observations 1771 -> 3147\n",
      "--- MAIL 28 STATES : \n",
      "State 1 observations 0 -> 2224\n",
      "State 2 observations 2225 -> 2540\n",
      "--- MAIL 29 STATES : \n",
      "State 1 observations 0 -> 2343\n",
      "State 2 observations 2344 -> 2889\n",
      "--- MAIL 30 STATES : \n",
      "State 1 observations 0 -> 2172\n",
      "State 2 observations 2173 -> 5159\n"
     ]
    }
   ],
   "source": [
    "def printPath(path):\n",
    "    summary = []\n",
    "    summary.append({ 'start': 0, 'end': 0, 'value': path[0]})\n",
    "    for i in range(1, len(path)):\n",
    "        if path[i] != path[i-1]:\n",
    "            summary.append({ 'start': i, 'end': i, 'value': path[i] })\n",
    "        else:\n",
    "            summary[-1]['end'] = i\n",
    "    \n",
    "    for s in summary:\n",
    "        print('State ' + str(s['value']) + ' observations ' \n",
    "              + str(s['start']) + ' -> ' + str(s['end']))\n",
    "\n",
    "distributions = np.loadtxt('p.text', dtype=float)\n",
    "\n",
    "for i in range(11,31):\n",
    "    observations = np.loadtxt('./dat/mail'+str(i)+'.dat', dtype=int)\n",
    "    p = viterbiAlgorithm(obs=observations, states=[1,2], start_prob=[1,0], trans=A, cond_prob=distributions)\n",
    "    print('--- MAIL ' + str(i) + ' STATES : ')\n",
    "    printPath(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Print the track and present and discuss the results obtained on mail11.txt to mail30.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem consistent, first of all because of their values: our HMM starts in State 1, goes to State 2 after a considerable number of characters and never comes back to State 1. \n",
    "\n",
    "Furthermore, I checked some emails and results are encouraging. For example, in mail11 our algorithm states that body starts at character 2850 while I found it beginning at character 2851: it could be my fault (I was using Atom editor to select characters and I'm not 100% sure of where it starts/ends selecting them), but the results is anyhow very precise! \n",
    "\n",
    "Obviously, there are some more significant errors, as in email14, where the model struggles a little to recognize the beginning of the body because it begins with a quote from a previous email that actually looks like a header. I think, however, that this is a limitation of the whole model: if I sent an email containing the header of another email in the body, the model will fail to distinguish between the real model and the copied header in the body.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How would you model the problem if you had to segment the mails in more than two parts (for example : header, body, signature) ?\n",
    "\n",
    "In this case, our the hidden model wouldn't change since the possible values of our observations (characters) remain the same: instead, it would change the observed model which will now have three states (namely header, body and signature). The transition matrix will hence become a 3x3 matrix of this form:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "    p_{11}       & p_{12} & 0 \\\\\n",
    "     0       & p_{22} & p_{23} \\\\\n",
    "     0       & 0 & 1\n",
    " \\end{bmatrix} $$\n",
    "\n",
    "and the initial vector π:\n",
    "\n",
    "$$π^{T} = (1, 0, 0)$$\n",
    "\n",
    "## 6. How would you model the problem of separating the portions of mail included, knowing that they always start with the character \">\".\n",
    "\n",
    "The model would now have four states, namely header, body_text, body_included, and signature, with a transition matrix 4x4 of this form:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "    p_{11}       & p_{12} & p_{13} & 0 \\\\\n",
    "     0       & p_{22} & p_{23} & p_{24} \\\\\n",
    "     0       & p_{32} & p_{33} & p_{34} \\\\\n",
    "     0       & 0 & 0 & 1\n",
    " \\end{bmatrix} $$\n",
    "\n",
    "and the initial vector π:\n",
    "\n",
    "$$\\pi^{T} = (1, 0, 0, 0)$$\n",
    "\n",
    "where body_text and body_include are now respectively state 2 and 3, while signature becomes state 4.\n",
    "\n",
    "The fact of knowing that included mail always start with a \">\" character is an information that regards only the outcome of the observation, and would therefore influence not the observation model but the hidden one: the conditional probability vector will have a higher probability at the place corresponding to character \">\" and state 3.\n",
    "\n",
    "More precisely, if the vector of conditional probabilities is built only by looking if a character is present or not in the desired portion of speech, this probability would be equal to 1 for the class corresponding to included mails: since included mails always start with the character \">\", the probability of having a \">\" conditioned to the fact of being in class body_included is 1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
